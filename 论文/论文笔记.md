# 笔记

## Ideas

1. 在多任务的情况下，寻求一个合适的并行化策略，使得整体的训练时间基本一致，并且能够快速完成。
2. 制定混合化的并行策略快速完成机器学习任务，比起单独使用其中一个都要快。
3. 参数同步方式（PS、AR、碎片化）。
4. 管道并行（wave、权重存储、权重预测）。
5. 前向传播算激活值，反向传播算梯度，参数更新用梯度更新参数。
6. 卷积层用数据并行，全连接层用模型并行。
7. 极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！
8. PS架构缺点慢➕不均衡已经逐渐被淘汰，对于在资源配置当中仍然可以使用该方式去探索最优资源配置。因为该架构下server和worker的分配方案对一些训练任务的影响最好和最坏情况相差十倍以上，并且不是说一味的追求server多或者worker多就能找到最优情况，因此PS架构适合探索最优资源配置。
9. 现在主流的框架都推荐在分布式训练上使用AllReduce架构，像Pytorch用的是Ring-AllReduce算法，TensorFlow会根据底层架构自动选择合适的算法，因为这个架构存在多种通信方式，难建模，这方面的研究还比较少。
10. 分布式训练的时候虽然加快了训练速度，但是存在场景就是GPU利用率可以达到90%以上，可是内存的占用却不到50%。

## 分布式机器学习集群的资源调度机制研究_王丹

0. 后续展望：在最大化模型性能时没有考虑任务的放置；在云上，要考虑成本问题；混合集群，既有生产级模型又有实验级模型时，如何考虑资源调度。
1. 预测收敛的迭代次数或者人工迭代次数，并收集在指定资源调度方案下的单次迭代时间，两者相乘即获得任务的训练完成时间。
2. 资源分配、资源位置、任务干扰都会影响到一个任务的训练速度。
3. 生产级模型和实验级模型，前者的目标是最小化任务完成时间，后者的目标是最大化模型性能，即通常希望**分配更多资源给最终得到的模型性能更高的训练任务**。
4. 在最小化任务完成时间的资源调度中，使用了参数服务器架构，使用算法在给定设备数量的前提下寻找server和worker的最优配置，例如设备数量为N，server为i，那么worker为N-i，在某种组合下，可以使任务训练速度达到最快。
5. 参数服务器架构的资源配置，在一些场景下，最好和最坏情况可能达到十倍以上的差距，并且两者并不是一比一的线性关系，因此在该架构下寻找最优资源配置有研究价值。
6. GPU是整体资源，是调度的最小单位，计算节点的“数量”及其“位置”构成了分布式的资源配置。
7. 所谓GPU位置敏感，就是指一批GPU在不在一个PCIe switch下，或者在一个CPU socket下，但是在不同的PCIe switch下，最后就是在不同的CPU socket下。

## PipeDream

0. 在复制阶段中使用的数据并行不是PyTorch当中的那种将输入数据继续拆分为更小批次的数据并行，而是不拆分输入数据，将输入数据轮流给到这些用于数据并行的设备上，比如batch1过来，由worker1去训练，计算到一半，此时batch2过来，交由worker2去计算。看起来像是数据并行化。
1. 卷积层权重少输出大，全连接层权重多输出小，数据并行适合卷积层多的模型，例如ResNet-50，但不适合LSTM或全连接层多的模型。因为参数多就是表示参数同步的时候开销会变大。
2. 管道并行的特点是可以极大地减少通信开销，这在参数大的模型训练或者通信带宽有限的环境下很有效。
