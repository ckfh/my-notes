# 笔记

## 未尝试

1. 数据并行训练可以和自动混合精度AMP一起使用。

## 设定每个应用程序的可见设备

CUDA_VISIBLE_DEVICES

## 训练次数

GoogleNet：1GPU训练，1次100，500次；4GPU训练，1个GPU100，125次。每step一次就需要通信同步参数。

## 通信后端

在Pytorch中支持三种分布式后端，分别是GLOO/MPI/NCCL，由于分布式训练是为了加快训练速度，通常选择在GPU上进行训练，与之匹配最好的通信后端是NCCL，Pytorch基于NCCL提供的运算方式实现了Ring-AllReduce算法进行参数同步。

## 分布式概览

分布式数据并行训练是一种广泛采用的单程序多数据训练模式。使用DDP，模型在每个进程中被复制，并且每个模型副本将获得不同的输入数据样本。DDP负责梯度通信以保持模型副本同步，并将其与梯度计算重叠以加快训练速度。

基于RPC的分布式训练是为了支持不能适应数据并行训练的通用训练结构而开发的，如分布式流水线并行、参数服务器模式以及DDP与其它训练模式的结合。它有助于管理远程对象生命周期，并将autograd引擎扩展到机器边界之外。

集体通信库支持跨组内的进程发送张量。它提供了集体通信API和点对点通信API，从1.6.0开始DDP和RPC都是从这个库中构建的，前者使用集体通信，后者使用点对点通信。一般开发人员不需要直接使用原始的通信API，上述DDP和RPC功能已经能满足大多数的开发场景。但是，在某些情况下，此API仍然有帮助，一个示例是分布式的参数平均，其中应用程序希望在反向传播后计算所有模型参数的平均值去代替使用DDP来传递梯度。这可以使得通信与计算脱钩，并允许对通信内容进行更细粒度的控制，但另一方面，它也放弃了DDP提供的性能优化。

1. 单设备训练（单进程单设备）
2. 数据并行训练（单进程多设备）易用但无法提供最佳性能，原因在于每次进行前向传播时都要进行模型拷贝，并且单进程多线程会争用GIL。
3. 分布式数据并行训练（多进程多设备）每个进程维护自己的优化器，并在每次迭代中执行一个完整的优化步骤。虽然这可能看起来是多余的，因为梯度已经被收集在一起并且在各过程中被平均，因此对于每个进程是相同的，这意味着不需要参数广播步骤，减少了在节点之间传输张量所花费的时间。每个进程都包含一个独立的Python解释器，消除了额外的解释器开销和“GIL抖动”，这是由于从一个Python进程中驱动多个执行线程、模型副本或图形处理器而产生的。这对于大量使用Python运行时的模型尤其重要，包括具有循环层或许多小组件的模型。
4. 分布式数据并行训练支持多机训练
5. 如果预计会发生显存溢出等错误，或者想在训练期间进行动态资源管理可以使用TORCHELASTIC

## 分布式数据并行内部设计

前提：因为使用了集体通信库，因此进程组的实例必须在分布式模型实例前被实例化。

构造：RANK0进程负责一开始的模型参数广播，确保所有进程中的模型副本都从完全相同的状态开始。每个进程都创建一个local Reducer，它将在反向传播过程中负责梯度同步。为了提高通信效率，Reducer将参数梯度放到buckets，并且一次减少一个bucket。可以在构造函数中设置bucket_cap_mb参数来配置bucket size。parameter gradients和buckets的映射是在构造时根据bucket size和parameter sizes确定的。模型参数按照给定模型的Model.parameters()相反的顺序分配到buckets。使用相反顺序的原因是因为DDP期望梯度在反向传播中以大致相同的顺序准备好。下图显示了一个示例。注意，梯度0和梯度1在桶1中，另外两个梯度在桶0中。当然，这个假设可能并不总是正确的，当这种情况发生时，它可能会损害DDP的反向传播速度，当Reducer不能在尽可能早的时间启动通信时。除了bucket，Reducer还在构造过程中注册autograd的hooks，one hook per parameter。当梯度准备好时，这些hook将在反向传播期间触发。（也就是说，参数同步时不是一个参数一个参数进行同步，而是一批参数一批参数进行同步。）

前向传播：DDP接受输入并将其传递给本地模型，然后分析来自本地模型的输出(如果find_unused_parameters设置为True)。这种模式允许在模型的子图上向后传递，DDP通过从模型输出中遍历autograd图并标记所有未使用的参数为准备归并的参数，从而找出向后传递中涉及的参数。在向后传递过程中，Reducer只等待未准备好的参数，但它仍然会归并所有的buckets。将参数梯度标记为ready并不能帮助DDP跳过当前bucket，但它将防止DDP在向后传递过程中永远等待缺少的梯度。注意，遍历autograd图会引入额外的开销，因此应用程序应该只在必要时将find_unused_parameters设置为True。

反向传播：backward()函数直接在loss Tensor上调用，这是不受DDP控制的，而DDP使用在构建时注册的autograd hooks来触发梯度同步。当一个梯度准备好了，它对应的梯度累加器上的DDP hook就会被触发，然后DDP就会标记参数梯度为准备归并。当一个bucket中的梯度都准备好了，Reducer在那个bucket上启动一个异步all-reduce来计算所有进程的梯度的平均值。当所有的bucket准备好后，Reducer将阻塞，等待所有的all-reduce操作完成。完成后，平均梯度被写入所有参数的param.grad字段。因此，在向后传递之后，不同DDP进程之间相同对应参数上的grad字段应该是相同的。

参数更新：从优化器的角度来看，它正在优化一个本地模型。所有DDP进程上的模型副本可以保持同步，因为它们都从相同的状态开始，并且在每次迭代中都有相同的平均梯度。

## 单机模型并行

注意层与张量要放置在对应的设备上，调用损失函数时要保证输出和标签位于同一设备上。

在反向传播和参数更新步骤中，这些分散在不同设备的层仿佛是在一个设备上。

## 分布式数据并行

各自维护有一个优化器，因此参数更新在各自的进程中进行，需要同步的只有反向传播当中计算出来的梯度，而数据并行则是全局优化器，必须收集所有梯度在一个设备上更新参数后，再把参数同步给其它设备。

在等级0上随机化初始参数后由等级0设备把参数广播出去，因此可以不需要设定种子函数来限定参数相同。

梯度同步通信发生在反向传播期间，并与梯度计算重叠。当backward()返回时，param.grad已经包含了同步梯度张量。之后交由优化器进行参数更新即可。

有时，由于网络延迟、资源竞争、不可预测的工作负载高峰等原因，进程之间不均衡的处理速度是不可避免的。为了避免在这些情况下超时，请确保在调用init_process_group时传递足够大的超时值。对GLOO后端来说这个超时值默认为30分钟。

在训练期间保存和加载检查点的一种优化方式是保存到一个进程然后加载到其它进程从而减少写开销。原因在于所有进程都是从相同参数开始训练并且梯度都是同步的，因此在一个进程中进行save操作即可。务必要确保所有进程加载前save操作已经完成。加载模型时需要提供map_location参数防止其它进程进入到不属于它的设备，如果不提供该参数，加载操作会先把模型加载到CPU然后复制每个参数到它保存过的位置，这会导致所有进程使用同一组设备。使用barrier()函数保证进程加载模型前该模型的save操作已完成。

world size/local world size，全局进程数、本地进程数；local rank/global rank，本地进程序号、全局进程序号。每个进程都需要知道自己的local rank和global rank，知道后，每个进程都会创建一个ProcessGroup，使其能够参与到集体通信当中。

## 在AWS实例上使用分布式数据并行

export NCCL_SOCKET_IFNAME=ens3 // 设置环境变量为私有IP的接口

在前传之前将数据和标签的non_block属性设置为True。这允许数据的异步GPU拷贝，这意味着传输可以与计算重叠。

batch_size是每个进程的batch_size，这意味着全局的batch_size是batch_size乘以world_size。

workers是每个进程中dataloaders使用的工作进程数，加快数据加载速度。

2个节点，每个节点8个GPU，如果一个进程对应一个GPU，则global rank范围是0-15，而local rank范围在2个节点上都为0-7。

## RPC

RPC支持使用给定的参数在指定的目标工作者上运行函数，并获取返回值或创建对返回值的引用。

如果用户代码在没有返回值的情况下无法继续，请使用同步API。否则，使用异步API获取future，并在调用者需要返回值时等待future。

remote()在需要远程创建某些内容但不需要将其获取给调用者时非常有用。比如一个进程远程调用了一个server和一个worker，这个进程可以在server创建一个内嵌表并且和worker共享，但是这个内嵌表永远不会在进程本地被使用。同步和异步API在这里就不再适用，因为这两个API要求返回值立即或在将来返回给调用者，这个调用者就是此处的进程。
