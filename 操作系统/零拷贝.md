# 零拷贝

[小林文章](https://juejin.cn/post/6875608045876543502)笔记。

## 为什么要有DMA技术

没有DMA技术之前，应用程序发起一次read系统调用的过程如下：

1. 用户进程发起read系统调用给CPU，从用户态切换为核心态；
2. CPU发起IO请求给磁盘；
3. 磁盘将数据放入磁盘控制器缓冲区里；
4. 磁盘发起IO中断信号给CPU；
5. CPU将数据从磁盘控制器缓冲区拷贝到`PageCache`；
6. CPU将数据从`PageCache`拷贝到用户缓冲区(磁盘控制器缓冲区=>PageCache=>用户缓冲区)；
7. read系统调用返回，从内核态切换为用户态。

整个过程中，CPU亲自参与数据搬运过程，无法进行其它操作。

DMA技术也叫直接内存访问，**在进⾏ I/O 设备和内存的数据传输的时候，数据搬运的⼯作全部交给DMA 控制器，⽽ CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**。

1. 用户进程发起read系统调用给CPU，从用户态切换为核心态；
2. CPU发起IO请求给DMA；
3. DMA发起IO请求给磁盘；
4. 磁盘将数据放入磁盘控制器缓冲区里；
5. 磁盘通知DMA控制器；
6. DMA将数据从磁盘控制器缓冲区拷贝到内核缓冲区；
7. DMA发出数据读完信号给CPU；
8. CPU将数据从内核缓冲区拷贝到用户缓冲区；
9. read系统调用返回，从内核态切换为用户态。

 整个数据传输的过程，CPU 不再参与数据搬运的⼯作，⽽是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪⾥传输到哪⾥，都需要 CPU 来告诉 DMA 控制器。

## 传统的文件传输有多糟糕？

假设服务端实现了文件传输功能，一个最简单的方式：将磁盘上的文件读取出来，然后通过网络协议栈发送给客户端。

```cpp
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

首先，发生了4次用户态到内核态的切换，因为有两次系统调用，调用前需要切换到核心态，调用结束后再切回用户态。

其次，发生了4次数据拷贝，2次是DMA拷贝，2次是CPU拷贝：

1. DMA把磁盘上的数据拷贝到内核缓冲区。
2. CPU把内核缓冲区里的数据拷贝到用户缓冲区。
3. CPU把用户缓冲区里的数据拷贝到内核socket缓冲区。
4. DMA把内核socket缓冲区里的数据拷贝到网卡缓冲区。

## 如何优化文件传输的性能？

要想提⾼⽂件传输的性能，就需要减少「⽤户态与内核态的上下⽂切换」和「内存拷⻉」的次数。

如何减少上下文切换次数？

读取磁盘数据的时候，之所以要发⽣上下⽂切换，这是**因为⽤户空间没有权限操作磁盘或⽹卡**，内核的权限最⾼，这些操作设备的过程都需要交由操作系统内核来完成，所以⼀般要通过内核去完成某些任务的时候，就需要使⽤操作系统提供的系统调⽤函数。

**要想减少上下⽂切换的次数，就要减少系统调⽤的次数**。

如何减少数据拷贝次数？

传统的⽂件传输⽅式会历经 4 次数据拷⻉，实际上，「从内核的读缓冲区拷⻉到⽤户的缓冲区⾥，再从⽤户的缓冲区⾥拷⻉到 socket 的缓冲区⾥」，这个过程是没有必要的。

因为⽂件传输的应⽤场景中，在⽤户空间我们并不会对数据「再加⼯」，所以数据实际上可以不⽤搬运到⽤户空间，**因此⽤户的缓冲区是没有必要存在的**。

## 如何实现零拷贝？

### mmap

read()系统调⽤的过程中会把内核缓冲区的数据拷⻉到⽤户的缓冲区⾥，于是为了减少这⼀步开销，用mmap()替换read()系统调用：

```cpp
buf = mmap(file, len);
write(sockfd, buf, len);
```

mmap() 系统调⽤函数会直接把内核缓冲区⾥的数据「**映射**」到⽤户空间，这样，操作系统内核与⽤户空间就不需要再进⾏任何的数据拷⻉操作。

1. DMA把数据拷贝到内核缓冲区，应用进程和内核共享此缓冲区。
2. CPU直接从内核缓冲区将数据拷贝到socket缓冲区。
3. DMA把socket缓冲区数据拷贝到网卡中。

4次上下文切换，因为两个系统调用函数。

3次数据拷贝，2次DMA，1次CPU。

### sendfile

针对文件发送，Linux有一个专门的系统调用函数sendfile()，它可以直接替代前面的read()和write()两个系统调用，减少了1次系统调用，也就减少了2次上下文切换。

其次，该系统调用不需要借助映射，可以直接把内核缓冲区数据拷贝到socket缓冲区，也不需要拷贝到用户态。

2次上下文切换。

3次数据拷贝，2次DMA，1次CPU。

### SG-DMA(全程仅依赖DMA技术)

如果网卡支持SG-DMA，可以进一步减少通过CPU把内核缓冲区拷贝到socket缓冲区的过程。

1. DMA把数据拷贝到内核缓冲区。
2. 缓冲区描述符和数据长度传到socket缓冲区，网卡的SG-DMA控制器可以直接将内核缓冲区里的数据拷贝到网卡缓冲区里。

2次上下文切换，sendfile()。

2次数据拷贝，1次普通DMA，1次SG-DMA。

这就是所谓的**零拷贝技术，因为我们没有在内存层⾯去拷⻉数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进⾏传输的**。

**零拷⻉技术可以把⽂件传输的性能提⾼⾄少⼀倍以上**。

### 使用零拷贝技术的项目

如果你追溯 Kafka ⽂件传输的代码，你会发现，最终它调⽤了 Java NIO 库⾥的 transferTo ⽅法，如果 Linux 系统⽀持 sendfile() 系统调⽤，那么 transferTo() 实际上最后就会使⽤到 sendfile() 系统调⽤函数。

## PageCache有什么作用？

- 缓存最近被访问数据；
- 预读功能。

回顾前⾯说道⽂件传输过程，其中第⼀步都是先需要先把磁盘⽂件数据拷⻉「内核缓冲区」⾥，这个「内核缓冲区」实际上是磁盘⾼速缓存（PageCache）。

我们都知道程序运⾏的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很⾼，于是我们可以⽤ PageCache 来缓存最近被访问的数据，当空间不⾜时淘汰最久未被访问的缓存。

还有⼀点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是⾮常耗时的，为了降低它的影响，PageCache 使⽤了「预读功能」。

⽐如，假设 read ⽅法每次只会读 32 KB 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后⾯的 32～64 KB 也读取到 PageCache，这样后⾯读取 32～64 KB 的成本就很低，如果在 32～64KB 淘汰出 PageCache 前，进程读取到它了，收益就⾮常⼤。

**但是，在传输⼤⽂件（GB 级别的⽂件）的时候，PageCache 会不起作⽤，那就⽩⽩浪费 DMA 多做的⼀次数据拷⻉，造成性能的降低，即使使⽤了 PageCache 的零拷⻉也会损失性能**。

这是因为如果你有很多 GB 级别⽂件需要传输，每当⽤户访问这些⼤⽂件的时候，内核就会把它们载⼊ PageCache 中，**于是 PageCache 空间很快被这些⼤⽂件占满**。

由于⽂件太⼤，可能某些部分的⽂件数据被再次访问的概率⽐较低，这样就会带来 2 个问题：

- PageCache 由于⻓时间被⼤⽂件占据，其他「热点」的⼩⽂件可能就⽆法充分使⽤到 PageCache，于是这样磁盘读写的性能就会下降了；
- PageCache 中的⼤⽂件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷⻉到 PageCache ⼀次；

**针对⼤⽂件的传输，不应该使⽤ PageCache，也就是说不应该使⽤零拷⻉技术，因为可能由于 PageCache 被⼤⽂件占据，⽽导致「热点」⼩⽂件⽆法利⽤到 PageCache，这样在⾼并发的环境下，会带来严重的性能问题**。

## 大文件传输用什么方式实现？

回到最开始的传统I/O模式，全程应用程序处于阻塞状态，对于阻塞问题，可以用异步I/O来解决。

1. 应用进程发起异步IO读给内核，直接返回执行其它任务。
2. 内核发起IO请求给磁盘。
3. 磁盘发起IO中断信号给内核。
4. 内核将数据从磁盘控制器缓冲区拷贝到用户缓冲区(**注意这里跳过了内核缓冲区**)。
5. 内核通知应用进程读取完毕，应用进程处理IO数据。

异步 I/O 并没有涉及到 PageCache，所以使⽤异步 I/O 就意味着要绕开 PageCache。

绕开 PageCache 的 I/O 叫直接 I/O，使⽤ PageCache 的 I/O 则叫非直接 I/O。通常，对于磁盘，异步 I/O 只⽀持直接 I/O。

**在⾼并发的场景下，针对⼤⽂件的传输的⽅式，应该使⽤「异步 I/O + 直接 I/O」来替代零拷⻉技术**。

直接 I/O 应⽤场景常⻅的两种：

- **应⽤程序已经实现了磁盘数据的缓存**，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；
- **传输⼤⽂件的时候**，由于⼤⽂件难以命中 PageCache 缓存，⽽且会占满 PageCache 导致「热点」⽂件⽆法充分利⽤缓存，从⽽增⼤了性能开销，因此，这时应该使⽤直接 I/O。
