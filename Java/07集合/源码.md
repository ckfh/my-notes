# 笔记

## ArrayList

ArrayList是在底层数组满时才进行扩容，也就是size属性+1后等于elementData.length时，每次扩容都扩容为elementData.length的1.5倍，调用Arrays.copyOf来创建一个新的数组，将旧数组元素转移到新数组上。

如果size+1-elementData.length>0时，就表示现在数组已满，无法放入一个新元素，需要进行扩容。

旧容量为elementData.length，新容量为elementData.length的1.5倍，如果新容量大于Integer.MAX_VALUE-8=2,147,483,639，需要二次确认新容量，传入size+1，如果大于2,147,483,639，选择2147483647作为新容量，否则选择2,147,483,639作为新容量。

**即size真的达到了2,147,483,639，那就选择最大值作为新容量，否则就暂时封顶1.5倍的最大值为2,147,483,639**。

一旦说size达到最大值2147483647，此时加1将导致循环溢出转为负数，将直接抛出OutOfMemoryError错误。

调用ensureCapacity(int minCapacity)方法，将进行扩容，扩容到至少可以容纳参数指定大小的长度，该方法可在添加大量元素之前使用，避免底层频繁地进行扩容。

## HashMap

HashMap在JDK7及之前的底层数据结构是数组加链表，也就是发生冲突时使用拉链法解决冲突。

在JDK8后还增加了红黑树，当数组长度大于64，并且当前链表的长度大于8时，就会选择将当前链表转化为红黑树，而当红黑树节点小于6时又会退化为链表。

对key进行hash：`(h = key.hashCode()) ^ (h >>> 16)`，`(h >>> 16)`目的是让高位也参与到计算当中，否则仅靠低位很容易造成冲突。

数组`Node<K,V>[] table`，传入的key、value，连同hash值被构造成一个Node对象，放入数组指定位置。

### 添加元素

先根据hash值和数组长度定位到数组位置`(n - 1) & hash`，若该位置没有其它元素，则创建一个Node元素放入该位置。

`(n - 1) & hash`这个实际是取模运算，因为hash值会超出数组长度，要对数组长度进行取模后才有对应的下标可以放入该元素。

若该位置已有其它元素，若就是同一个key，则直接覆盖value。

若不是同一个key，即发生冲突，如果当前元素是TreeNode类型(虽然是Node引用)，强转并调用putTreeVal方法将待插入数据插入到红黑树中。

如果当前元素不是TreeNode类型，仍旧是Node类型，表明插入数据将作为链表节点，遍历到链表尾部(JDK7采用头插法)，新建Node节点并插入。

如果链表节点数量超过8，转化为红黑树(此时数组长度应该大于64，否则只会扩容数组)，若遍历链表过程中遇到相同key，仍会直接覆盖value。

在转化为红黑树之前，先尝试扩容(即数组长度小于64时)，此时待转换为红黑树的链表会被重新hash到两个链表中，长度减小，不需要进行红黑树转换。

注意：如果存在覆盖情况，会将原来的旧值进行返回。

### 获取元素

根据hash定位到数组位置，如果当前桶只有一个元素，或就是桶的第一个元素，直接返回当前元素。

不在桶的第一个元素，且桶不止一个元素时，如果是红黑树，就在树中获取，如果是链表，那就遍历获取。

### 移除元素

根据hash定位到数组位置，桶不为空，如果就在首位，记录下来，如果不在就到链表或红黑树中找，最后根据是桶的首位，还是链表，还是红黑树，分别用不同的方法进行删除。

如果红黑树节点被删除至阈值6，将重新转换为链表结构。

### 数组扩容

**会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的**。

注意扩容的阈值是数组长度乘以加载因子，也就是在达到数组3/4容量时就进行扩容。

原数组容量超过最大值1073741824(1<<30)，不再扩容，任由元素进行碰撞，没超过则扩容为原来的2倍，与之对应的扩容阈值也为原来的2倍。

扩容后，将旧的键值对转移到新的哈希桶数组中，如果旧桶无下一个元素，则根据(newCap - 1) & hash确定新的位置并赋值。

有下一个元素，如果是红黑树，会拆成两个子链表，再分别按需转成红黑树，如果是链表，拆成两个子链表并保持原有顺序。

这是因为新的hash对扩容后的数组长度求余不再是同一个地点，例如97和105在长度为8时，取余都为1，但是在翻倍后，取余前者为1，后者为9，此时已经应该位于两个不同的链表当中，其中一个链表依旧为原来的位置，另一个链表的位置为原来的位置加上原来的数组长度。

经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。

因此，我们在扩充HashMap的时候，不需要像JDK7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。

这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK8新增的优化点。

HashMap和ArrayList，size表示的是已有元素的个数，和底层数组的实际长度不一定匹配。

扩容是一个特别耗性能的操作，所以在初始化HashMap的时候，**要注意有加载因子的存在，即扩容阈值并不等于数组长度**，估算大小，初始化的时候给一个大致的数值，避免频繁扩容。

负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。

一个定义初始容量的Bad Case如下：

```java
HashMap hashMap = new HashMap(2);
hashMap.put("1", 1);
hashMap.put("2", 2);
hashMap.put("3", 3);
```

当hashMap设置最后一个元素3的时候，会发现当前的哈希桶数组大小已经达到扩容阈值2*0.75=1.5，紧接着会执行一次扩容操作，因此，此类的代码每次运行的时候都会进行一次扩容操作，效率低下。在日常开发过程中，**一定要充分评估好HashMap的大小，尽可能保证扩容的阈值大于存储元素的数量，减少其扩容次数**。

### 扰动函数

`(h = key.hashCode()) ^ (h >>> 16)`和`(n - 1) & hash`。

加入扰动函数的理由是，取余运算用到了&运算，因为位运算直接对内存数据进行操作，不需要转成十进制，因此处理速度非常快，这样就导致位运算&效率要比取模运算%高很多。

但是单纯只有与操作时，由于数组长度较小，其转换为二进制时，高16位皆为0，这就会导致一些大hash，高位不同，低位相同时造成哈希冲突，因为高位被归零，只看低位取值。

因此利用扰动函数，将高16位左移后和原先的低16位进行^操作，使得高位的一些特征也对低位产生印象，使得低位随机性加强，降低发生冲突的概率。

而先进行扰动，再进行与运算，将数据右位移16位，哈希码的高位和低位混合了起来，这也正解决了前面所讲高位归0，计算只依赖低位最后几位的情况，这使得高位的一些特征也对低位产生了影响，使得低位的随机性加强，能更好的避免冲突。

[掘金参考](https://juejin.cn/post/6931521369209470989#heading-27)

### 长度是2的幂次的原因

即使指定了初始容量，也会选择一个可以包含该初始容量的2的幂次作为初始容量。

这是因为对数组长度进行求余操作选择的是&运算不是%运算，而对除数是2的幂次的%操作等价于与其除数减一的&操作，所以hash & (n - 1)。

这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。

### JDK7并发死链问题

主要问题在于该版本链表的插入操作采用的是头插法，就是在第一个线程刚拿到链表节点准备进行将其移动到新table上时被挂起，另一个线程完成了rehash过程，第一个线程继续向下执行时形成了循环单链表。

[酷壳参考](https://coolshell.cn/articles/9606.html#并发下的Rehash)

虽然JDK8使用了尾插法来保证扩容后和扩容前节点是一致的顺序(想一想一个链表重新被头插法后的顺序)，也不代表在多线程下能够安全扩容，会出现比如扩容丢数据等问题。

### 为什么链表转化红黑树的阈值是8

如果hashCode的分布离散良好的话，那么红黑树是很少会被用到的，因为各个值都均匀分布，很少出现链表很长的情况。

在理想情况下，链表长度符合泊松分布，各个长度的命中概率依次递减，源码注释中给我们展示了1-8长度的具体命中概率，当长度为8的时候，概率概率仅为0.00000006，这么小的概率，HashMap的红黑树转换几乎不会发生，因为我们日常使用不会存储那么多的数据。

[SF参考](https://segmentfault.com/a/1190000023112171)

## 红黑树

## 并发容器类

Concurrent类型的容器内部很多操作使用CAS优化，一般可以提供较高吞吐量。

弱一致性问题：

1. 遍历时弱一致性，例如，当利用迭代器遍历时，如果容器发生修改，迭代器仍然可以继续进行遍历，这时内容是旧的。
2. 求大小弱一致性，size操作未必是100%准确。
3. 读取弱一致性。

遍历时如果发生了修改，对于非安全容器来讲，使用fail-fast机制也就是让遍历立刻失败，抛出ConcurrentModificationException，不再继续遍历。

## ConcurrentHashMap

### JDK7

对于同一个Segment的操作才需考虑线程同步，不同的Segment则无需考虑。

一个ConcurrentHashMap维护一个Segment数组，一个Segment维护一个HashEntry数组。

#### 构造方法

concurrencyLevel决定了最大并发数，默认为16，最大为65536。

HashEntry数组初始容量为initialCapacity，默认为16，最大为1073741824(1<<30)。

构造方法中，2的sshift次幂等于ssize，ssize就是Segment数组的长度，由concurrencyLevel计算得出。

ssize是大于等于concurrencyLevel的2的幂次。

segmentShift = 32 - sshift和segmentMask = ssize - 1用于定位Segment。

实例化时只会创建一个Segment，后续的Segment会延迟创建。

#### put方法

ConcurrentHashMap不允许K/V为空。

hash后无符号右移segmentShift和segmentMask进行与操作，定位Segment。

如果定位的Segment未初始化，则初始化，然后调用Segment的put方法，向HashEntry数组放入键值对，

**Segment继承自ReentrantLock，过程中会使用tryLock尝试加锁，和unlock进行解锁**。

**获取锁失败会调用scanAndLockForPut自旋获取锁，重试次数超过MAX_SCAN_RETRIES时会直接使用lock方法阻塞式获取锁，保证在其它线程释放后能够获取锁**。

#### get方法

get方法无需加锁，由于其中涉及到的共享变量都使用volatile修饰，volatile可以保证内存可见性，所以不会读取到过期数据。

### JDK8

K/V不能为空。

抛弃了原有的Segment分段锁，而采用了CAS+Synchronized来保证并发安全性。

初始化数组时，使用sizeCtl变量表示当前的初始化状态：

1. -1，说明另外的线程CAS操作成功，当前线程让出CPU使用权。
2. -N，说明有N-1个线程正在进行扩容。
3. table初始化后，表示table容量，此时是阈值，即一个16大小的数组，此时sizeCtl是12。

#### put方法

定位桶，桶为空，CAS放入，成功则跳出返回；桶不为空，synchronized加锁加入节点。

## 几个并发Map的区别

HashTable中采用了**全表锁**，即所有操作均上锁，串行执行，采用synchronized关键字修饰。这样虽然保证了线程安全，但是在多核处理器时代也极大地影响了计算性能，这也致使HashTable逐渐淡出开发者们的视野。

针对HashTable中锁粒度过粗的问题，在JDK8之前，ConcurrentHashMap引入了**分段锁**机制。在原有结构的基础上拆分出多个segment，每个segment下再挂载原来的entry（上文中经常提到的哈希桶数组），每次操作只需要锁定元素所在的segment，不需要锁定整个表。因此，锁定的范围更小，并发度也会得到提升。

虽然引入了分段锁的机制，即可以保证线程安全，又可以解决锁粒度过粗导致的性能低下问题，但是对于追求极致性能的工程师来说，这还不是性能的天花板。因此，在JDK8中，ConcurrentHashMap摒弃了分段锁，使用了**乐观锁**的实现方式。放弃分段锁的原因主要有以下几点：

- 使用segment之后，会增加ConcurrentHashMap的存储空间。
- 当单个segment过大时，并发性能会急剧下降。

ConcurrentHashMap在JDK8中的实现废弃了之前的segment结构，沿用了与HashMap中的类似的Node数组结构。

ConcurrentHashMap中的乐观锁是采用synchronized+CAS进行实现的。

### ConcurrentHashMap的put方法再解释

当put的元素在哈希桶数组中不存在时，则直接CAS进行写操作。

这里涉及到了两个重要的操作，tabAt与casTabAt。可以看出，这里面都使用了Unsafe类的方法。Unsafe这个类在日常的开发过程中比较罕见。我们通常对Java语言的认知是：Java语言是安全的，所有操作都基于JVM，在安全可控的范围内进行。然而，Unsafe这个类会打破这个边界，使Java拥有C的能力，可以操作任意内存地址，是一把双刃剑。这里使用到了ASHIFT，来计算出指定元素的起始内存地址，再通过getObjectVolatile与compareAndSwapObject分别进行取值与CAS操作。

在获取哈希桶数组中指定位置的元素时为什么不能直接get而是要使用getObjectVolatile呢？

因为在JVM的内存模型中，每个线程有自己的工作内存，也就是栈中的局部变量表，它是主存的一份copy。因此，线程1对某个共享资源进行了更新操作，并写入到主存，而线程2的工作内存之中可能还是旧值，脏数据便产生了。Java中的volatile是用来解决上述问题，保证可见性，任意线程对volatile关键字修饰的变量进行更新时，会使其它线程中该变量的副本失效，需要从主存中获取最新值。**虽然ConcurrentHashMap中的Node数组是由volatile修饰的，可以保证可见性，但是Node数组中元素是不具备可见性的。因此，在获取数据时通过Unsafe的方法直接到主存中拿，保证获取的数据是最新的**。

当put的元素在哈希桶数组中存在，并且不处于扩容状态时，则使用synchronized锁定哈希桶数组中第i个位置中的第一个元素f（头节点2），接着进行double check，类似于DCL单例模式的思想。校验通过后，会遍历当前冲突链上的元素，并选择合适的位置进行put操作。此外，ConcurrentHashMap也沿用了HashMap中解决哈希冲突的方案，链表+红黑树。这里只有在发生哈希冲突的情况下才使用synchronized锁定头节点，其实是比分段锁更细粒度的锁实现，只在特定场景下锁定其中一个哈希桶，降低锁的影响范围。
