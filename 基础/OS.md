# 笔记

## CPU64位和32位的问题

64 位相比 32 位 CPU 的优势主要体现在两个方面：

- 64 位 CPU 可以一次计算超过 32 位的数字，而 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进行计算，效率就没那么高，但是大部分应用程序很少会计算那么大的数字，所以只有运算大数字的时候，64 位 CPU 的优势才能体现出来，否则和 32 位 CPU 的计算性能相差不大。
- 64 位 CPU 可以寻址更大的内存空间，32 位 CPU 最大的寻址地址是 4G，即使你加了 8G 大小的内存，也还是只能寻址到 4G，而 64 位 CPU 最大寻址地址是 2^64，远超于 32 位 CPU 最大寻址地址的 2^32。

64 位和 32 位软件，实际上代表指令是 64 位还是 32 位的：

- 如果 32 位指令在 64 位机器上执行，需要一套兼容机制，就可以做到兼容运行了。但是如果 64 位指令在 32 位机器上执行，就比较困难了，因为 32 位的寄存器存不下 64 位的指令；
- 操作系统其实也是一种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，比如 64 位操作系统，指令也就是 64 位，因此不能装在 32 位机器上。

**总之，硬件的 64 位和 32 位指的是 CPU 的位宽，软件的 64 位和 32 位指的是指令的位宽**。

## 写出让CPU跑得更快的代码

如果内存中的数据已经在 CPU Cache 中了，那 CPU 访问一个内存地址的时候，会经历这 4 个步骤：

1. 根据内存地址中索引信息，计算在 CPU Cache 中的索引，也就是找出对应的 CPU Line 的地址；
2. 找到对应 CPU Line 后，判断 CPU Line 中的有效位，确认 CPU Line 中数据是否是有效的，如果是无效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；
3. 对比内存地址中组标记和 CPU Line 中的组标记，确认 CPU Line 中的数据是我们要访问的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执行；
    - 如果采用直接映射，同一个CPU Line会被映射多个内存块，也就是这几个内存块如果加载到Cache中都是放在同一个位置上，因此需要一个**组标记**来进行区分。
4. 根据内存地址中偏移量信息，从 CPU Line 的数据块中，读取对应的字。

### 如何更快(如何写出CPU缓存命中率高的代码)

**命中缓存**，这是最简单的提升代码性能的方式。

L1 CACHE被分为**数据缓存**和**指令缓存**，因此可以从这两个方面来看缓存的命中率。

数据缓存命中率提升：

- 二维数组是先行后列快，还是先列后行快？
    - 先行后列快，因为二维数组占用的内存是连续的，内存中元素的布局先是按行，再是按列来存放数据。
    - **因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利用 CPU Cache 带来的好处，这样我们代码的性能就会得到很大的提升**。

指令缓存命中率提升：

案例：对一个随机数数组进行两项操作，一是排序，二是遍历把小于50的数组元素置为0(if语句)。

- 是先遍历再排序快，还是先排序再遍历快？(先排序再遍历快)

**分支预测器**：对于 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是 if 还是 else 中的指令。那么，**如果分支预测可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中**，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快。

**当数组中的元素是随机的，分支预测就无法有效工作，而当数组元素都是顺序的，分支预测器会动态地根据历史命中数据对未来进行预测，这样命中率就会很高**。

### 多核CPU怎么说

现代 CPU 都是多核心的，进程可能在不同 CPU 核心来回切换执行，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核心之间共享的，但是 L1 和 L2 Cache 都是每个核心独有的，如果一个进程在不同核心来回切换，各个核心的缓存命中率就会受到影响，相反如果进程都在同一个核心上执行，那么其数据的 L1 和 L2 Cache 的缓存命中率可以得到有效提高，缓存命中率高就意味着 CPU 可以减少访问 内存的频率。

当有多个同时执行「**计算密集型**」的线程，为了防止因为切换到不同的核心，而导致缓存命中率下降的问题，我们可以把**线程绑定在某一个 CPU 核心上**，这样性能可以得到非常可观的提升。

## CPU是如何选择当前执行线程的

Linux中，根据任务的优先级和响应要求，分为两种，优先级数值越小，优先级越高。

- 实时任务，对系统响应事件要求很高，尽可能地执行实时任务，优先级在0-99范围内就算实时任务。
- 普通任务，优先级在100-139都是普通任务级别。

### 调度类

由于任务存在优先级，Linux为了保证高优先级任务尽可能地被执行，分了几种调度类：deadline、realtime、fair。

前二者都是应用于实时任务的，它们的调度策略合起来有三种：sched-deadline、sched-fifo、sched-rr。

- sched-deadline：按照deadline调度。
- sched-fifo：先按优先级再按到达时间，高优先级可以抢占。
- sched-rr：相同优先级会被轮流运行，每个任务都有一定的**时间片**，高优先级依旧可以抢占低优先级。

第三者用于普通任务，两种调度策略：sched-normal、sched-batch。

- sched-normal：普通任务的调度策略。
- sched-batch：后台任务的调度策略，不和终端交互，不影响其它需要交互的任务，可以适当降低优先级。

### 完全公平调度

基本上遇到的都是普通任务，对于普通任务来说，公平性最重要。

完全公平调度算法的理念是想让分配给每个任务的 CPU 时间是一样，于是它为每个任务安排一个虚拟运行时间 vruntime，如果一个任务在运行，其运行的越久，该任务的 vruntime 自然就会越大，而没有被运行的任务，vruntime 是不会变化的。

虽然是普通任务，但是普通任务之间还是有优先级区分的，所以在计算虚拟运行时间 vruntime 还要考虑普通任务的**权重值**，注意权重值并不是优先级的值，内核中会有一个 nice 级别与权重值的转换表，nice 级别越低的权重值就越大。

高权重任务的 vruntime 比低权重任务的 vruntime **少**，少才会被完全公平调度算法选择。

### CPU运行队列

每个CPU都有自己的运行队列，分为三个运行队列：deadline、realtime、csf，csf使用红黑树来描述的，按vruntime大小排序，最左侧叶子节点，就是下次被调度的任务(二叉搜索树，左根右)。

会按照deadline、realtime、csf的顺序从运行队列中选择任务进行调度。

### 调整优先级

如果你想让某个普通任务有更多的执行时间，可以调整任务的 nice 值，从而让优先级高一些的任务执行更多时间。nice 的值能设置的范围是 -20～19， 值越低，表明优先级越高，因此 -20 是最高优先级，19 则是最低优先级，默认优先级是 0。

## 缓存一致性和伪共享问题

要想实现CPU缓存的一致性，必须满足：1.写传播，2.事务的串行化。基于总线嗅探机制的MESI协议，满足了这两点要求，它是一个保证缓存一致性的协议。

为了保证CPU缓存一致性而导致的缓存失效问题，也被称为伪共享问题，解决伪共享问题的方法就是通过填充的方式，将除了指定数据以外的CPU LINE区域进行填充，填充的是一些不会被读写的LONG类型变量，防止伪共享问题的产生。

## 中断

**中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度的影响**。

中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说中断有可能会丢失，所以中断处理程序要短且快。

Linux 系统**为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」**。

- **上半部用来快速处理中断**，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。
- **下半部用来延迟处理上半部未完成的工作**，一般以「内核线程」的方式运行。

比如，常见的网卡接收网络包的例子。

网卡收到网络包后，会通过**硬件中断**通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来响应该事件，这个事件的处理也是会分成上半部和下半部。

上部分要做到快速处理，所以只要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态，比如把状态更新为表示数据已经读到内存中的状态值。

接着，内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。

- 上半部直接处理硬件请求，也就是硬中断，主要是负责耗时短的工作，特点是快速执行；
- 下半部是由内核触发，也就说软中断，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；

还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为「ksoftirqd/CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 ksoftirqd/0。

## Reactor和Proactor

两者的区别用一句话就可以总结：看读写数据需不需要自己来，要自己来的就是reactor，否则就是proactor。

- Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- Proactor 是异步网络模式， 感知的是已完成的读写事件。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。

因此，Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件，这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。

无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件。

## 缓存一致性

为了保证CPU Cahe和内存的数据一致性，CPU Cache的写入分为两类：1.写直达，2.写回。

写直达：不管数据是否存在于cache block中，都会写入到内存里。

写回：先写cache block，当被修改过的cache block要被替换出去时再写回内存。

解决不同CPU核心cache的缓存一致性：1.写传播，2.事务的串行化。

写传播：当某个核心在cache里更新了数据，需要同步到其它核心的cache里。

事务的串行化：就是多个核心同时更新数据，在同步到其它核心里的时候要按照相同的次序进行更新。

总线嗅探：每当核心更新了cache就发一个广播消息到总线，其它核心都会监听该消息，并判断是否有相同的cache需要置为无效。

可以发现，总线嗅探方法很简单，CPU需要每时每刻监听总线上的一切活动，但是不管别的核心的Cache是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。

另外，总线嗅探只是保证了某个CPU核心的Cache更新数据这个事件能被其他CPU核心知道，但是并不能保证事务串形化。

有一个协议基于总线嗅探机制实现了事务串形化，也用状态机机制降低了总线带宽压力，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存一致性。

MESI协议：M，已修改，E，独占，S，共享，I，已失效。

如果是独占或者已修改状态的cache line，更新数据时不需要发送广播给其它CPU核心。

一个核心读取一个数据之前，会发送消息给其它核心，如果其它核心已经持有了该数据，则双方的cache line状态需要修改为共享。

## 进程、线程、协程

进程是最初的概念。

程序并不能单独运行，只有将程序装载到内存中，系统为它分配资源才能运行，而这种执行的程序就称之为进程。

程序和进程的区别就在于：程序是指令的集合，它是进程运行的静态描述文本；进程是程序的一次执行活动，属于动态概念。

在多道编程中，我们允许多个程序同时加载到内存中，在操作系统的调度下，可以实现并发地执行。这是这样的设计，大大提高了CPU的利用率。进程的出现让每个用户感觉到自己独享CPU，因此，进程就是为了在CPU上实现多道编程而提出的。

有了进程为什么还要线程。

进程有很多优点，它提供了多道编程，让我们感觉我们每个人都拥有自己的CPU和其他资源，可以提高计算机的利用率。

其实，仔细观察就会发现进程还是有很多缺陷的，主要体现在两点上：

- 进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。
- 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。

除了提高进程的并发度，线程还有个好处，就是可以有效地利用多处理器和多核计算机。现在的处理器有个趋势就是朝着多核方向发展，在没有线程之前，多核并不能让一个进程的执行速度提高，原因还是上面所述的两点限制。但如果将一个进程分解为若干个线程，则可以让不同的线程运行在不同的核上，从而提高了进程的执行速度。

多核并不能让进程效率加快，因为进程就是运行在单核上的，而一个进程的多个线程则可以运行在多核上，共同提高进程的效率。

多线程在单进程内部，多协程在单线程内部。

## 页面置换算法

- OPT：优先淘汰最长时间内不会被访问的页面。
    - 缺页率最小，性能最好，但无法实现。
- FIFO：优先淘汰最先进入内存的页面。
    - 实现简单，但性能很差，可能出现Belady异常(当为进程分配的物理块数增大时，缺页次数不减反增的异常现象。)。
- LRU：优先淘汰最近最久没访问的页面。
    - 性能很好，但需要硬件支持，算法开销大。
- CLOCK(NRU)：循环扫描各页面，第一轮淘汰访问位=0的，并将扫描过的页面访问位改为1。若第一轮没选中，则进行第二轮扫描。
    - 实现简单，算法开销小，但未考虑页面是否被修改过。
- 改进型CLOCK(改进型NRU)：若用（访问位, 修改位）的形式表述，则第一轮：淘汰（0, 0），第二轮：淘汰（0, 1），并将扫描过的页面访问位都置为0，第三轮：淘汰（0, 0）第四轮：淘汰（0, 1）。
    - 算法开销较小，性能也不错。

## 进程调度

进程调度算法的评价标准：

- CPU利用率=忙碌的时间/总时间
- 系统吞吐率=总共完成了多少作业/总共花了多少时间
- 周转时间
    - 周转时间=作业完成时间-作业提交时间
    - 平均周转时间=各作业周转时间之和/作业数
    - 带权周转时间=作业周转时间/作业实际运行时间
    - 平均带权周转时间=各作业带权周转时间之和/作业数
- 等待时间
    - 等待时间=等待被服务的时间之和
    - 平均等待时间=各个作业等待时间的平均值
- 响应时间=从用户提交请求到首次产生响应所用的时间

进程调度算法：

先来先服务

- 优点：公平，实现简单
- 缺点：对长作业有利，对长作业之后的短作业不利。

不会导致饥饿，因为长作业迟早也有运行结束的一刻，此时短作业就可以得到运行。

短作业优先

有可抢占版本。

- 优点：”最短的“平均等待和周转时间。
- 缺点：不公平，对短作业有利，长作业不利。可能产生饥饿现象，另外运行时间由用户提供，不一定真实，不一定能做到真正的短作业优先。

高响应比优先

响应比=(等待时间+要求服务时间)/要求服务时间

综合考虑了等待时间和运行时间(要求服务时间)，等待时间相同时，要求服务时间短的优先(短作业优先的优点)，要求服务时间相同时，等待时间长的优先(先来先服务的优点)，对于长作业来说，随着等待时间越来越就，响应比越来越大，从而避免了长作业饥饿的问题。

## 指令流水线

指令流水线是为提高处理器执行指令的效率，把一条指令的操作分成多个细小的步骤，每个步骤由专门的电路完成的方式。

例如一条指令要执行要经过3个阶段：取指令、译码、执行，每个阶段都要花费一个机器周期，如果没有采用流水线技术，那么这条指令执行需要3个机器周期；如果采用了指令流水线技术，那么当这条指令完成“取指”后进入“译码”的同时，下一条指令就可以进行“取指”了，这样就提高了指令的执行效率。

## 死锁

互斥条件：该资源任意一个时刻只由一个线程占用。(这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）)

请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。(一次性申请所有的资源)

不剥夺条件：线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。(占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源)

循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。(靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件，**最常用**)

## I/O多路复用

一个进程虽然任一时刻只能处理一个请求，但是在处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个CPU并发多个进程，所以也叫做时分多路复用。

### select/poll

它们都只会告诉你有几个通道准备好了，但是不会告诉你具体是哪几个通道。所以，一旦知道有通道准备好以后，自己还是需要进行一次扫描。

将文件描述符集合从用户态拷贝到内核态，内核态遍历检查是否有网络事件发生，如果有事件发生，则标记为可读或者可写，然后将集合从内核态再拷贝回用户态，用户态遍历检查找到可读或者可写的socket，然后进行处理。(2次遍历➕2次拷贝)

select的文件描述符集合使用有长度限制的bitmap，而poll使用链表来组织，没有长度限制。

两者并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

### epoll

epoll是如何解决select和poll限制的？

1. 在内核里使用**红黑树来跟踪进程所有待检测的文件描述符**，把每个需要监控的socket通过epoll_ctl()函数加入到红黑树里，对红黑树进行操作，就不需要像select/poll每次都把整个socket集合传入，减少了内核和用户空间大量的数据拷贝和内存分配。
2. 采用事件驱动机制，内核里**维护了一个链表记录就绪事件**，当某个socket有事件发生，通过“回调函数”将其加入到就绪事件列表中，当用户调用epoll_wait()函数时，只会将有事件发生的socket文件描述符的个数拷贝回用户空间，不需要像select/poll遍历整个socket集合，提高了检测效率。

epoll能监听的最大socket数量取决于操作系统定义的进程能打开的最大文件描述符个数。

能直接返回具体的准备好的通道。

在内核中用红黑树跟踪所有待检测的文件描述符，把需要监控的socket添加到红黑树中，epoll基于事件驱动，内核里维护了一个链表记录就绪事件，当某个socket有事件就绪，通过回调函数将其加入到链表当中，当用户调用epoll_wait()时，只会返回有事件就绪的文件描述符。

而就绪链表仍需要从内核态拷贝到用户态再进行遍历。

epoll支持两种事件触发方式：边缘触发和水平触发。

边缘触发（edge-triggered，ET）：当监控的socket有可读事件发生时，服务器只会从epoll_wait()中苏醒一次，即使进程本身没有调用read函数从内核中读取数据，也依然只苏醒一次，因此要保证程序一次性将内核缓冲区的数据读取完。

如果使用边缘触发方式，同一个I/O事件只会触发一次，而且不知道能对内核读写多少数据，所以收到通知后要尽可能地读写数据，以免错失读写的机会，而造成数据的丢失。需要**循环**从文件描述符读写数据，如果文件描述符是**阻塞**的，没有数据可读写，进程会阻塞在读写函数那里，程序就无法向下运行。所以，**边缘触发一般配合非阻塞I/O搭配(等待数据准备完成后再一次性进行读写)**。

水平触发（level-triggered，LT）：当监控的socket有可读事件发生时，服务器不断地从epoll_wait()中苏醒，直到内核缓冲区数据被read函数读取完才结束，目的是告诉我们有数据需要读取。

水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。

**就是说边缘触发对于同一个可读事件只会提醒一次，而水平触发对于同一个可读事件会不断提醒，直到该可读事件被处理**。

一般来说，边缘触发的效率比水平触发的效率要高，因为**边缘触发可以减少epoll_wait的系统调用次数**，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

**JavaNIO底层用的就是水平触发模式，事件发生后，要么处理，要么取消，否则下次该事件仍会触发**。

select/poll只有水平触发模式，epoll默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

**使用I/O多路复用时，最好搭配非阻塞I/O一起使用，因为多路复用API返回的事件并不一定可读写的，如果使用阻塞I/O，那么在调用read/write时则会发生程序阻塞，因此最好搭配非阻塞I/O，以便应对极少数的特殊情况**。

## I/O模型

I/O模型分为两大类：

1. 同步I/O(阻塞I/O、非阻塞I/O、基于非阻塞I/O的多路复用)
2. 异步I/O

阻塞等待需要等待两个步骤：1.内核将数据准备好。2.数据从内核态拷贝到用户态。

阻塞I/O就是应用进程步骤1和2都需要等待。

非阻塞I/O就是应用进程可以跳过步骤1只需要等待步骤2，但是执行步骤2之前会不断轮询数据是否已经准备好。

基于非阻塞I/O的多路复用也是跳过步骤1只需要等待步骤2，但是它不需要自己轮询数据是否准备好，而是由内核来通知进程数据是否准备好。

上面三个步骤的共同特点就是，将数据从内核态拷贝到用户态这一步骤都需要进程自己来完成，因此被分类为同步I/O。

而异步I/O就是拷贝数据这一步骤也不需要，内核直接帮你拷贝到位，然后通知进程，进程直接读取数据即可。

## 进程通信

1. 管道(匿名/有名)
2. 消息队列
3. 共享内存
4. socket(跨主机)
5. 信号量用来实现进程同步
6. 信号用来向进程发出控制指令

## 王道笔记

### 进程

- 进程互斥，为临界区代码前后增加一对PV操作。
- 进程同步，遵循“前V后P”的原则，即先执行的进程对某个信号量进行V操作，后执行的进程对相同信号量进行P操作。
- 多生产和消费指的是抢占的资源多样，不是生产和消费者多个。
- 生产者-消费者问题展示了如何进行进程之间的互斥与同步操作。
- 多生产者-消费者问题展示了进程之间对多类型资源的互斥与同步操作。
- 吸烟者问题展示了“可生产多种产品的单生产者-多消费者”问题。
- 读者-写者问题展示了利用计数器来解决复杂的进程互斥问题。
- 哲学家进餐问题展示了如何避免进程因持有多个临界资源而导致的死锁问题。
- 引入管程的目的无非就是要更方便地实现进程互斥和同步。
- 发生死锁时一定有循环等待，有循环等待时不一定死锁，其意思在于，如果某个进程实际上有可替代的同类资源，它只要去申请可用的同类资源，就可以解除循环等待状态。
- 不安全状态不一定会发生死锁，其意思在于如果不安全状态下没有进程提出导致死锁的请求就不会发生死锁，而发生了死锁则一定处于不安全状态。
- 在资源分配之前预先判断这次分配是否会导致系统进入不安全状态，以此决定是否答应资源分配请求。这也是“银行家算法”的核心思想。
- 死锁避免和死锁预防的区别：死锁预防是设法破坏产生死锁的四个必要条件之一，严格防止死锁的出现；而死锁避免则不那么严格地限制产生死锁的必要条件存在。因为即使死锁的必要条件存在，也不一定发生死锁，死锁避免是在系统运行过程中注意避免死锁的最终发生。

### 内存

- 进程：PCB（进程信息）|程序段（指令）|数据段（数据）
- 内存地址和存储单元的关系，内存是由一个个存储单元组成的，而内存地址从0开始，每个地址对应一个存储单元。
- 现代操作系统利用动态重定位的方式来实现程序逻辑地址到物理地址的转换，所谓动态重定位是指在运行包含逻辑地址的指令时，将逻辑地址与起始地址相加得到物理地址，而起始地址则存放在重定位寄存器中。
- 中级调度(内存调度)：决定将哪个处于挂起态的进程重新调入内存运行，这需要该进程在被挂起时，保留进程PCB，并在PCB当中记录外存当中的地址，这样才能够在需要调度时重新调入。
- 内部碎片是指分配给需要10MB内存的进程20MB的内存空间，多出的10MB内存空间就是内部碎片。
- 外部碎片是指内存中某些空闲分区由于太小而难以利用。
- 分页、分段、段页都是内存空间的非连续分配管理方式。
- 分页就是将内存空间划分为固定大小的页，这些页用来存储进程的一部分，因此页比起固定分区当中分区大小要小的多。
- 一个用户进程被分为多个页，每个页都有一个页号，且其对应到内存空间中的内存块号/页框号。
- 一个进程对应一张页表，进程的每一页对应页表当中的一个页表项，每个页表项由“页号”和“块号”组成，页表记录了进程页面和实际存放的内存块之间的对应关系。
- 页表项长度相同，且页号是“隐含”的。我们只要知道页表存放的起始地址和页表项的长度，即可找到各个页号对应的页表项存放的位置。
- 基本地址变换机构会有两次访问内存的操作，第一次是访问页表拿到内存块号，第二次是访问目标内存单元。
- 快表就是增加一个寄存器，用于记录最近使用过的页表项，这是基于程序运行的局部性原理提出的概念。
- 如果快表命中，则直接访问内存当中的存储单元，未命中则先访问内存中的页表，再访问存储单元。
- 单级页表的缺点在于需要连续内存来分配页表，这和内存空间的离散分配管理方式是有冲突的，无法发挥其优点，且由程序的局部性原理，我们没必要让整个页表都常驻内存。
- 因此将单级页表离散化，按页面大小分组，为了记录分组后的单级页表，需要一个页目录表记录，这种结构也称为两级页表，而对于局部性原理的解决方式，则是在需要访问目标页面的时候，才将目标页面从外存调入内存（只有在内存当中才可以通过CPU利用指令查找内存块号），如果访问的目标页面不在内存时，这种情况被称为缺页中断，然后从外存调入内存。
- 二级页表访存逻辑地址换算流程：0号|1号|偏移量，为了访问一级页表的0号页表项，需要在一级页表的起始物理地址的基础上加上页表项索引乘以页表项长度得到0号页表项的起始物理地址，从该物理地址开始取出页表项大小的数据，该数据存放的是一个块号，这是二级页表所在的块号，因此根据该块号找到二级页表的起始地址，再根据起始地址加上页表项索引乘以页表项长度得到1号页表项的起始物理地址，从该物理地址开始取出页表项大小的数据，该数据存放的是一个块号，这是存储单元所在的块号，根据块号的起始地址加上偏移量即可得到目标存储单元的物理地址，从物理地址当中取出指令或运算数据。
- 段长指的是一个段的大小为多少，通过段内地址的位数可知最大段长，而通过物理内存大小可知基址的编码位数，两个拼接在一起就可以得到段表项的长度。
- 通过段号找到基址，再根据基址和段内偏移计算实际物理地址。
- 根据从进程PCB当中恢复段表寄存器当中的内容，即段表起始地址|段表长度，当执行某条逻辑指令时，里面包含了逻辑地址A，它的内容是段号|段内地址，根据段号和段表长度判断是否越界，如果没有越界则根据起始地址加上段表项长度乘以段号，拿到对应的段表项，此时再根据段内地址是否超出段长，如果没有超出段长，则根据段表项当中的基址，以及段内地址得到存储单元的实际物理地址。
- 分页地址空间是一维的，分段地址空间是二维的，这是从用户的角度所看到的，这是因为后者分段后，每个分段的起始地址都是0，因此既需要段号也需要段内地址，而前者即使分页后，整个内存的地址就是从0开始到结束，因此用户只要给出一个逻辑地址就能访问其中的一个存储单元。
- 段页式存储将采用一个段表寄存器用于存放段表起始地址以及段表长度，进程执行某条指令，指令当中包含逻辑地址，该逻辑地址的结构为段号|页号|页内偏移量，首先利用段表长度判断段号是否越界，之后根据段表起始地址，加上段表项长度乘以段号得到该段号对应的段表项的物理地址，取出段表项，该段表项中存放有页表长度|页表存放块号，利用页表长度判断页号是否越界，之后根据页表存放块号乘以页面大小得到该段的页表起始地址，再加上页号乘以页表项长度得到页表项的物理地址，取出页表项，该页表项中存放着内存块号，再根据内存块号和页内偏移量得到存储单元的物理地址。
- 段页式存储是二维的，用户只要给出段号和段内偏移，而系统会将段内偏移拆分为页号和页内偏移。
- 连续分配和非连续分配都是传统存储管理的方式，它们结合交换和覆盖技术能提高内存利用率以及从逻辑上扩充内存容量。
- 虚拟内存就是为了解决传统存储管理的一次性和驻留性问题，基于程序的局部性原理提出的一种技术。
- 时间局部性：指令可能再次访问；空间局部性：存储单元可能再次访问。
- 为了实现虚拟内存技术，操作系统需要提供请求调页(或请求调段)，这是当访问信息不在内存时，需要从外存调入内存，而页面置换(或段置换)功能，是当内存不够时，将暂时用不到的信息换到外存。

### 文件

- 计算机所有的操作都是在CPU上执行的，例如打开文件这个操作，系统会将文件所在目录调入内存得到目录信息，然后检索对应文件名，再将对应文件调入内存并打开，可以想象成打开文件这个操作本质上也是一个进程在CPU上执行，例如Windows的文件资源管理器，会在CPU当中建立相应的进程。
- 文件的物理结构(文件分配方式)是指文件数据应该怎样存放在外存中。
- 显式链接支持随机访问的意思是支持物理块的随机访问，虽然查询FAT表是顺序查，但是查询过程中并不需要真正访问实际的外存物理块，等待找到目标物理块时再进行外存访问。
- 很重要的一点是FAT表在开机时就会被读取并常驻内存。
- 文件是要被读取到内存当中操作的。
- 磁盘的盘面被划分成一个个磁道，一个个磁道又被划分成一个个扇区，一个扇区就是一个”磁盘块“，各个扇区存放的数据量相同，因此最内侧的扇区数据密度是最大的。
- 读写扇区必须由磁头划过相应的扇区。
- 盘面其实有多个，相互叠加，每个盘面上都有磁头在负责读写扇区，所有的磁头都是连在同一个磁壁上的，因此是”共进退“的形式。
- 所有盘面中对应的一圈圈磁道组成了一个柱面。
- (柱面号，盘面号，扇区号)定位任意一个”磁盘块“。
- CPU是不参与I/O操作的，但它会在I/O操作之前将相应的指令存放到I/O控制器中，由I/O控制器完成对IO设备的操作。
- 因此当进程需要等待IO操作的结果时，就不需要将其放在CPU上等待，而是阻塞该进程。
- 程序控制方式和中断驱动方式都需要CPU寄存器来作为中介，因此引出了DMA直接存储器存取方式。
- 前二者都是一个字一个字的输入和输出，后者是一个块一个块的输入和输出。

## 大端存储和小端存储

- 大端存储：将字节高位放置于低地址，将字节低位放置于高地址。
- 小端存储：将字节低位放置于低地址，将字节高位放置于低地址。

地址高低：0x00到0x03，就是地址从低到高。
字节高低：0x12345678，从后向前字节由低到高。

**如果字节按照大端模式进行存储，则其存放方式符合人类的阅读思维**。
